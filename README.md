# ğŸ¦ ApexTrust Bank â€“ Banking Lakehouse Project

## Overview

This project demonstrates a **real-world Banking Data Lakehouse** implementation using **Databricks, Apache Spark, and Delta Lake**. It is designed to showcase **hands-on data engineering skills**, **architecture decision-making**, and **exam-aligned best practices**.

The project follows the **Bronzeâ€“Silverâ€“Gold** data modeling pattern and focuses on **ACID transactions, auditability, CDC handling, and BI-ready analytics**, which are critical in regulated domains like banking.

---

## ğŸ¯ Project Goals

* Build a **Lakehouse architecture** on cloud-style object storage
* Practice **Delta Lake operations** (UPDATE, DELETE, MERGE, Time Travel)
* Apply **data quality and validation rules**
* Create **BI-ready analytical tables**
* Demonstrate **interview-ready, job-relevant skills**

---

## ğŸ§± Architecture

```
Raw Files (CSV)
      â†“
Bronze Layer (Raw Delta â€“ Audit)
      â†“
Silver Layer (Validated Delta)
      â†“
Gold Layer (Analytics Delta)
```

**Why Lakehouse?**

* Supports **structured & semi-structured data**
* Enables **ACID transactions on data lakes**
* Allows **BI, analytics, and ML on the same data**
* Reduces system complexity and data duplication

---

## ğŸ—‚ï¸ Git Repository Structure (Exam + Real Project Ready)

```
apextrust-bank-lakehouse/
â”‚
â”œâ”€â”€ README.md
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ customers.csv
â”‚   â”‚   â”œâ”€â”€ accounts.csv
â”‚   â”‚   â”œâ”€â”€ transactions.csv
â”‚   â”‚   â”œâ”€â”€ branches.csv
â”‚   â”‚   â””â”€â”€ cdc_customers.csv
â”‚
â”œâ”€â”€ unity-catalog/
â”‚   â””â”€â”€ create_catalog_and_schemas.sql
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_bronze_ingestion/
â”‚   â”‚   â””â”€â”€ bronze_ingestion.ipynb
â”‚   â”‚
â”‚   â”œâ”€â”€ 02_silver_processing/
â”‚   â”‚   â””â”€â”€ silver_processing.ipynb
â”‚   â”‚
â”‚   â””â”€â”€ 03_gold_analytics/
â”‚       â””â”€â”€ gold_analytics.ipynb
â”‚
â”œâ”€â”€ sql/
â”‚   â”œâ”€â”€ merge_operations.sql
â”‚   â”œâ”€â”€ time_travel_queries.sql
â”‚   â””â”€â”€ optimize_vacuum.sql
â”‚
â””â”€â”€ docs/
    â”œâ”€â”€ architecture.md
    â”œâ”€â”€ exam_notes.md
    â””â”€â”€ data_dictionary.md
```

---

## ğŸ“Š Data Model

### Core Tables

* **customers** â€“ customer master data with KYC status
* **accounts** â€“ bank accounts and balances
* **transactions** â€“ debit/credit transactions
* **branches** â€“ branch and regional mapping

These tables are processed across **Bronze, Silver, and Gold layers** using Delta Lake.

---

## ğŸ” Key Engineering Practices Demonstrated

### Delta Lake

* Delta table creation
* INSERT / UPDATE / DELETE
* MERGE (CDC handling)
* Time Travel for audit & recovery
* Schema enforcement and evolution

### Spark

* DataFrame transformations
* Spark SQL for analytics
* Join strategies for fact/dimension data

### Governance & Performance

* Bronze/Silver/Gold separation
* Audit-friendly raw data preservation
* OPTIMIZE and ZORDER for performance

---


